{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Dataframe Manipulation library\n",
    "import numpy as np #Data Manipulation library\n",
    "\n",
    "#sklearn modules for Feature Extraction & Modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Libraries for Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import joblib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(os.getcwd(),'data','News_Category_Dataset_v3.json'), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_combine'] = df['headline'] + \" \" + df['short_description']\n",
    "df = df[['text_combine','category']]\n",
    "df_train = df.iloc[:int(len(df)*0.8)]\n",
    "df_test = df.iloc[int(len(df)*0.8):len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combine</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_combine   category\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS\n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS\n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY\n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING\n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# nltk.download('wordnet',os.getcwd()) # for download to this path\n",
    "# nltk.download('punkt',os.getcwd())\n",
    "nltk.data.path.append('corpora')\n",
    "nltk.data.path.append('tokenizers')\n",
    "\n",
    "try:\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize(text : str) -> str:\n",
    "        return lemmatizer.lemmatize(text)\n",
    "except Exception as e:\n",
    "    print(f'failed to load WordNetLemmatizer {e}')\n",
    "    def lemmatize(text : str) -> str:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "except Exception as e:\n",
    "    print(f'error load nltk tokenize {e}')\n",
    "    def word_tokenize(text:str)->list:\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text:str)->str:\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def list_to_text(l:list)->str:\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text : str) -> str:\n",
    "    res = []\n",
    "    text = text_cleaning(text)\n",
    "    list_text = word_tokenize(text)\n",
    "    for word in list_text:\n",
    "        res.append(lemmatize(word))\n",
    "    return list_to_text(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'over million american roll up sleeve for omicrontargeted covid booster health expert said it is too early to predict whether demand would match up with the million dos of the new booster the u ordered for the fall'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text('Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Local\\Temp\\ipykernel_8444\\4197579947.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['text_combine_cleaned'] = df_train['text_combine'].apply(lambda x:preprocess_text(x))\n"
     ]
    }
   ],
   "source": [
    "df_train['text_combine_cleaned'] = df_train['text_combine'].apply(lambda x:preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(os.path.join(os.getcwd(),'data','local_train_data_clean.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combine</th>\n",
       "      <th>category</th>\n",
       "      <th>text_combine_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>over million american roll up sleeve for omicr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>american airline flyer charged banned for life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>of the funniest tweet about cat and dog this w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>the funniest tweet from parent this week sept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>woman who called cop on black birdwatcher lose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_combine   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                text_combine_cleaned  \n",
       "0  over million american roll up sleeve for omicr...  \n",
       "1  american airline flyer charged banned for life...  \n",
       "2  of the funniest tweet about cat and dog this w...  \n",
       "3  the funniest tweet from parent this week sept ...  \n",
       "4  woman who called cop on black birdwatcher lose...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "from time import time\n",
    "from gensim import utils\n",
    "import multiprocessing\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in df_train['text_combine_cleaned']:\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()\n",
    "model_w2v = gensim.models.Word2Vec(workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.09 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "model_w2v.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.91 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "model_w2v.train(sentences, total_examples=model_w2v.corpus_count, epochs=10, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83572507, -1.017963  ,  1.636769  ,  0.1990658 , -1.3724602 ,\n",
       "       -0.88695705, -3.222287  , -1.8108013 ,  0.9759952 ,  0.6300123 ,\n",
       "        0.8913318 ,  1.4861679 , -0.16220964, -0.7392116 ,  0.974985  ,\n",
       "       -0.62273157,  1.1330959 , -3.6353168 , -0.1328214 ,  2.3720486 ,\n",
       "       -0.96801776,  2.019842  ,  2.315325  , -1.1727276 ,  2.1405778 ,\n",
       "        2.0043626 , -1.9028642 ,  0.01106686, -1.6374978 ,  0.8556392 ,\n",
       "       -1.6430027 ,  0.41253135, -1.9979649 ,  0.6195668 ,  0.24608758,\n",
       "        0.9664026 , -3.0348446 ,  1.758334  ,  1.650371  , -0.96401733,\n",
       "        0.65161794,  0.44954973, -2.5018058 ,  0.44874212, -2.3940525 ,\n",
       "       -0.11602435, -4.312738  , -0.4812549 ,  0.7615709 ,  0.5544056 ,\n",
       "       -1.1617304 ,  0.45244548, -1.330128  ,  0.84189576,  3.4854383 ,\n",
       "       -0.32123047, -1.5706046 , -2.1290686 ,  0.09419014,  0.09356831,\n",
       "       -1.1354595 , -0.99886656, -0.39520872, -1.2296982 ,  0.18212397,\n",
       "        0.80739874, -0.12394285, -1.1035398 ,  1.7466639 , -0.1269324 ,\n",
       "       -4.3251634 , -1.8407261 , -0.7280469 ,  0.6204448 , -0.15874343,\n",
       "        1.0567361 , -1.0593135 , -0.7808569 , -1.7948492 , -1.3734895 ,\n",
       "        2.6663327 , -0.23688826,  1.4621153 ,  0.6267303 ,  2.2145414 ,\n",
       "        0.7565724 , -1.3600634 ,  0.6751665 ,  1.3669182 ,  0.18191841,\n",
       "        0.3454209 , -1.0674771 ,  4.608492  ,  0.9236993 , -1.3182324 ,\n",
       "        3.3051765 , -0.60081947,  0.49439347,  1.1061717 , -1.6954925 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv['million']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('billion', 0.826045572757721),\n",
       " ('trillion', 0.6207946538925171),\n",
       " ('thousand', 0.6041485667228699),\n",
       " ('percent', 0.5907055735588074),\n",
       " ('upwards', 0.581097424030304),\n",
       " ('dollar', 0.5791403651237488),\n",
       " ('hundred', 0.5787340998649597),\n",
       " ('taxpayer', 0.5559016466140747),\n",
       " ('multimillion', 0.5528634786605835),\n",
       " ('uninsured', 0.5505988001823425)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.wv['million']\n",
    "model_w2v.wv.most_similar(positive=[\"million\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43469036"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.similarity(\"million\", 'money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'europe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.doesnt_match([\"million\", \"money\", \"europe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(os.getcwd(),'saved_model')):\n",
    "    os.makedirs(os.path.join(os.getcwd(),'saved_model'))\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(),'saved_model','word2vec')):\n",
    "    os.makedirs(os.path.join(os.getcwd(),'saved_model','word2vec'))\n",
    "\n",
    "model_w2v.save(os.path.join(os.getcwd(),'saved_model','word2vec','gensim-word2vec-model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = gensim.models.Word2Vec.load(os.path.join(os.getcwd(),'saved_model','word2vec','gensim-word2vec-model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('billion', 0.826045572757721),\n",
       " ('trillion', 0.6207946538925171),\n",
       " ('thousand', 0.6041485667228699),\n",
       " ('percent', 0.5907055735588074),\n",
       " ('upwards', 0.581097424030304),\n",
       " ('dollar', 0.5791403651237488),\n",
       " ('hundred', 0.5787340998649597),\n",
       " ('taxpayer', 0.5559016466140747),\n",
       " ('multimillion', 0.5528634786605835),\n",
       " ('uninsured', 0.5505988001823425)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.wv.most_similar(positive=[\"million\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_df_train = []\n",
    "count = 0\n",
    "for sentence in df_train['text_combine_cleaned']:\n",
    "    tagged_df_train.append(gensim.models.doc2vec.TaggedDocument(sentence.split(), [count]))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=10, workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v.build_vocab(tagged_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'million' appeared 2593 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'million' appeared {model_d2v.wv.get_vecattr('million', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v.train(tagged_df_train, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model_d2v.infer_vector(df_train['text_combine_cleaned'].iloc[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15462857,  0.1212381 , -0.06696589,  0.15095988, -0.03678479,\n",
       "        0.04467931, -0.05589783,  0.2544154 , -0.36791766, -0.24292438,\n",
       "        0.21923864, -0.2611676 ,  0.02897439,  0.05075863, -0.18680346,\n",
       "       -0.02482542, -0.06713432, -0.08495195, -0.18180478, -0.17106368,\n",
       "       -0.00327925,  0.0983294 ,  0.03952096,  0.08551869, -0.11946163,\n",
       "        0.17738105, -0.06116995, -0.07073114,  0.03264059, -0.11202175,\n",
       "        0.01450013,  0.07382273,  0.14536524,  0.16590743,  0.21189901,\n",
       "       -0.03710944,  0.20411941,  0.04205662, -0.00130913, -0.10735402,\n",
       "        0.07143053,  0.00561398, -0.0595555 ,  0.23168136,  0.18159258,\n",
       "       -0.11564403, -0.1706933 , -0.23234123, -0.06277809,  0.05156521],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(tagged_df_train)):\n",
    "    inferred_vector = model_d2v.infer_vector(tagged_df_train[doc_id].words)\n",
    "    sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import collections\n",
    "\n",
    "# counter = collections.Counter(ranks)\n",
    "# print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(tagged_df_train[doc_id].words)))\n",
    "# print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model_d2v)\n",
    "# for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "#     print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(tagged_df_train[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakamin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
