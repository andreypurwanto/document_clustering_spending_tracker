{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Dataframe Manipulation library\n",
    "import numpy as np #Data Manipulation library\n",
    "\n",
    "#sklearn modules for Feature Extraction & Modelling\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#Libraries for Plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import joblib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(os.path.join(os.getcwd(),'data','News_Category_Dataset_v3.json'), lines=True)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_combine'] = df['headline'] + \" \" + df['short_description']\n",
    "df = df[['text_combine','category']]\n",
    "df_train = df.iloc[:int(len(df)*0.8)]\n",
    "df_test = df.iloc[int(len(df)*0.8):len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combine</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_combine   category\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS\n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS\n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY\n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING\n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet') # for download to this path\n",
    "nltk.download('punkt')\n",
    "nltk.data.path.append('corpora')\n",
    "nltk.data.path.append('tokenizers')\n",
    "\n",
    "try:\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def lemmatize(text : str) -> str:\n",
    "        return lemmatizer.lemmatize(text)\n",
    "except Exception as e:\n",
    "    print(f'failed to load WordNetLemmatizer {e}')\n",
    "    def lemmatize(text : str) -> str:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.tokenize import word_tokenize\n",
    "except Exception as e:\n",
    "    print(f'error load nltk tokenize {e}')\n",
    "    def word_tokenize(text:str)->list:\n",
    "        return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text:str)->str:\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def list_to_text(l:list)->str:\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text : str) -> str:\n",
    "    res = []\n",
    "    text = text_cleaning(text)\n",
    "    list_text = word_tokenize(text)\n",
    "    for word in list_text:\n",
    "        res.append(lemmatize(word))\n",
    "    return list_to_text(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'over million american roll up sleeve for omicrontargeted covid booster health expert said it is too early to predict whether demand would match up with the million dos of the new booster the u ordered for the fall'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text('Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrey\\AppData\\Local\\Temp\\ipykernel_31492\\4197579947.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['text_combine_cleaned'] = df_train['text_combine'].apply(lambda x:preprocess_text(x))\n"
     ]
    }
   ],
   "source": [
    "df_train['text_combine_cleaned'] = df_train['text_combine'].apply(lambda x:preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(os.path.join(os.getcwd(),'data','local_train_data_clean.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combine</th>\n",
       "      <th>category</th>\n",
       "      <th>text_combine_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>over million american roll up sleeve for omicr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>american airline flyer charged banned for life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>of the funniest tweet about cat and dog this w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>the funniest tweet from parent this week sept ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>woman who called cop on black birdwatcher lose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_combine   category  \\\n",
       "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "\n",
       "                                text_combine_cleaned  \n",
       "0  over million american roll up sleeve for omicr...  \n",
       "1  american airline flyer charged banned for life...  \n",
       "2  of the funniest tweet about cat and dog this w...  \n",
       "3  the funniest tweet from parent this week sept ...  \n",
       "4  woman who called cop on black birdwatcher lose...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "from time import time\n",
    "from gensim import utils\n",
    "import multiprocessing\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in df_train['text_combine_cleaned']:\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MyCorpus()\n",
    "model_w2v = gensim.models.Word2Vec(workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.11 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "model_w2v.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 1.17 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "model_w2v.train(sentences, total_examples=model_w2v.corpus_count, epochs=10, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.67070889e+00,  2.97764629e-01, -3.87172191e-03, -4.15316433e-01,\n",
       "       -2.21456575e+00, -1.45508075e+00, -2.70006227e+00, -3.54158354e+00,\n",
       "        9.09223735e-01,  9.96697068e-01,  4.24754173e-01,  1.63320243e+00,\n",
       "       -1.81076229e-01, -9.81446087e-01,  1.66050291e+00, -5.83251238e-01,\n",
       "       -7.58398592e-01, -2.54763675e+00,  1.32229006e+00,  2.54999399e+00,\n",
       "       -1.59519756e+00,  9.42233428e-02,  1.20004356e+00,  1.77716827e+00,\n",
       "        2.40126562e+00,  2.42855096e+00, -2.75748777e+00,  2.74765402e-01,\n",
       "       -1.85393643e+00,  1.62261510e+00, -1.60162663e+00,  6.00381136e-01,\n",
       "       -1.54121566e+00, -6.10493183e-01,  4.96884555e-01,  2.60938287e+00,\n",
       "       -2.46851373e+00,  8.68205070e-01,  2.25233245e+00, -2.17981029e+00,\n",
       "       -1.02518928e+00, -3.68876696e-01, -2.83617884e-01, -9.72736120e-01,\n",
       "       -1.72846043e+00, -2.68135995e-01, -3.52674460e+00, -1.60081840e+00,\n",
       "       -1.12326443e-01, -6.68782532e-01, -3.44077969e+00,  1.97472858e+00,\n",
       "       -2.56467968e-01,  7.29316771e-01,  2.78343153e+00, -8.11399996e-01,\n",
       "       -1.69011164e+00, -2.27694654e+00,  1.06829286e-01,  5.23481131e-01,\n",
       "       -1.58208668e+00,  8.68583083e-01,  1.18566787e+00, -1.64047909e+00,\n",
       "        1.39944360e-01, -9.08809721e-01, -5.70635855e-01, -2.56597829e+00,\n",
       "        7.48101175e-01, -6.27994120e-01, -2.94505286e+00, -1.98259604e+00,\n",
       "        3.46557766e-01, -8.61060560e-01,  8.60137045e-01,  2.66122627e+00,\n",
       "       -1.72081530e+00, -6.78562462e-01, -1.85487974e+00, -1.11159992e+00,\n",
       "        1.77401304e+00, -1.03959107e+00,  5.71144462e-01,  1.12834580e-01,\n",
       "        1.31500304e+00,  1.90014768e+00, -3.16886008e-01,  1.95954418e+00,\n",
       "        4.95893568e-01,  1.08157799e-01,  9.58222210e-01, -5.26115656e-01,\n",
       "        4.52436972e+00,  9.85580683e-01, -1.41102657e-01,  1.75379908e+00,\n",
       "       -4.30532902e-01, -4.81188983e-01,  1.27684438e+00,  1.02637410e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv['million']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('billion', 0.8345927000045776),\n",
       " ('percent', 0.622978925704956),\n",
       " ('trillion', 0.6179170608520508),\n",
       " ('thousand', 0.6094152927398682),\n",
       " ('hundred', 0.596312403678894),\n",
       " ('multimillion', 0.5870281457901001),\n",
       " ('dollar', 0.5757449865341187),\n",
       " ('approximately', 0.5638946294784546),\n",
       " ('charity', 0.556996762752533),\n",
       " ('taxpayer', 0.5466832518577576)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.wv['million']\n",
    "model_w2v.wv.most_similar(positive=[\"million\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37195346"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.similarity(\"million\", 'money')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'europe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.doesnt_match([\"million\", \"money\", \"europe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(os.getcwd(),'saved_model')):\n",
    "    os.makedirs(os.path.join(os.getcwd(),'saved_model'))\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(),'saved_model','word2vec')):\n",
    "    os.makedirs(os.path.join(os.getcwd(),'saved_model','word2vec'))\n",
    "\n",
    "model_w2v.save(os.path.join(os.getcwd(),'saved_model','word2vec','gensim-word2vec-model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = gensim.models.Word2Vec.load(os.path.join(os.getcwd(),'saved_model','word2vec','gensim-word2vec-model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('billion', 0.8345927000045776),\n",
       " ('percent', 0.622978925704956),\n",
       " ('trillion', 0.6179170608520508),\n",
       " ('thousand', 0.6094152927398682),\n",
       " ('hundred', 0.596312403678894),\n",
       " ('multimillion', 0.5870281457901001),\n",
       " ('dollar', 0.5757449865341187),\n",
       " ('approximately', 0.5638946294784546),\n",
       " ('charity', 0.556996762752533),\n",
       " ('taxpayer', 0.5466832518577576)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.wv.most_similar(positive=[\"million\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_df_train = []\n",
    "count = 0\n",
    "for sentence in df_train['text_combine_cleaned']:\n",
    "    tagged_df_train.append(gensim.models.doc2vec.TaggedDocument(sentence.split(), [count]))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=10, workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v.build_vocab(tagged_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'million' appeared 2593 times in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Word 'million' appeared {model_d2v.wv.get_vecattr('million', 'count')} times in the training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v.train(tagged_df_train, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model_d2v.infer_vector(df_train['text_combine_cleaned'].iloc[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19170862,  0.13979508, -0.22436477, -0.00709117, -0.05876046,\n",
       "       -0.0101215 ,  0.13376978,  0.45984292, -0.27711463, -0.14896072,\n",
       "        0.02321006, -0.14625786,  0.17133518,  0.22200453, -0.01037575,\n",
       "       -0.20642774, -0.03902422, -0.19873174, -0.24897559, -0.12321483,\n",
       "       -0.01739778,  0.01084521,  0.17079864,  0.06789946, -0.14071313,\n",
       "       -0.03883867, -0.18509503, -0.13101707, -0.03815425, -0.12378145,\n",
       "        0.04625451,  0.02155207, -0.04854417,  0.16249017,  0.0481233 ,\n",
       "        0.07740457,  0.00474954,  0.04811189, -0.01997378,  0.11343069,\n",
       "       -0.06328455, -0.02949234,  0.13160086,  0.11624507,  0.2014222 ,\n",
       "       -0.11299323, -0.20481513, -0.19465661, -0.08726005,  0.37011474],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(tagged_df_train)):\n",
    "    inferred_vector = model_d2v.infer_vector(tagged_df_train[doc_id].words)\n",
    "    sims = model_d2v.dv.most_similar([inferred_vector], topn=len(model_d2v.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (0): «over million american roll up sleeve for omicrontargeted covid booster health expert said it is too early to predict whether demand would match up with the million dos of the new booster the u ordered for the fall»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t11>:\n",
      "\n",
      "MOST (50434, 0.7549108862876892): «drug price senior program may deliver blow to state budget state may face billion in new medicaid cost next year by michael ollove higher prescription drug price combined»\n",
      "\n",
      "SECOND-MOST (153382, 0.7225825786590576): «of the best dessert recipe of all time were warning you these recipe are epic»\n",
      "\n",
      "MEDIAN (110335, 0.3055989742279053): «doomed airasia flight black box found»\n",
      "\n",
      "LEAST (85287, -0.5359851121902466): «way to fix baltimore police problem a few “ bad apple ” can change the culture of a whole police department honorable cop in baltimore feel pressured to honor»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(tagged_df_train[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model_d2v)\n",
    "for label, index in [('MOST', 1), ('SECOND-MOST', 2), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(tagged_df_train[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.\n",
      "Mylan Offers Discounts On EpiPen After Clinton Criticism The $600 list price of the drug will remain the same, but the company said it would increase the maximum copay assistance program to $300 from $100.\n",
      "SEC Commissioners Rejected Settlement With ITT Tech CEO The U.S. Securities and Exchange Commission has rejected a proposed settlement of the SEC’s lawsuit against two former executives\n",
      "Deepak Chopra Takes Shot At Donald Trump Spiritual leader and Super Genes author Deepak Chopra joined HuffPost Rise recently and said the biggest solution happening\n"
     ]
    }
   ],
   "source": [
    "print(df_train.iloc[0]['text_combine'])\n",
    "print(df_train.iloc[58388]['text_combine'])\n",
    "print(df_train.iloc[26343]['text_combine'])\n",
    "print(df_train.iloc[83245]['text_combine'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petrol cars cheaper diesel cars\n",
      "diesel cheaper petrol\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "doc1=\"petrol cars cheaper diesel cars\"\n",
    "doc2=\"diesel cheaper petrol\"\n",
    "doc_corpus=[doc1,doc2]\n",
    "for x in doc_corpus:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter((doc1+' '+doc2).split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_temp = list(counter.keys())\n",
    "res_temp = []\n",
    "count = 0\n",
    "for sentence in doc_corpus:\n",
    "    res_temp.append([])\n",
    "    counter_temp = Counter(sentence.split())\n",
    "    for key in counter:\n",
    "        if key in counter_temp:\n",
    "            res_temp[count].append(counter_temp[key])\n",
    "        else:\n",
    "            res_temp[count].append(0)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>petrol</th>\n",
       "      <th>cars</th>\n",
       "      <th>cheaper</th>\n",
       "      <th>diesel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   petrol  cars  cheaper  diesel\n",
       "0       1     2        1       1\n",
       "1       1     0        1       1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res_temp,columns=columns_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names n ['cars' 'cheaper' 'diesel' 'petrol']\n"
     ]
    }
   ],
   "source": [
    "vec=TfidfVectorizer(stop_words='english')\n",
    "vec.fit(doc_corpus)\n",
    "matrix = vec.transform(doc_corpus)\n",
    "print(\"Feature Names n\",vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85135433 0.30287281 0.30287281 0.30287281]\n",
      " [0.         0.57735027 0.57735027 0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = Total number of documents available\n",
    "# t = term for which idf value has to be calculated\n",
    "# df(t) = Number of documents in which the term t appears\n",
    "\n",
    "# idf(t) = log e [ (1+n) / ( 1 + df(t) ) ] + 1\n",
    "\n",
    "# Here n=2 (no. Of docs)\n",
    "\n",
    "# for d1\n",
    "\n",
    "# idf(“cars”) = log e (3/2) +1 => 1.405465083\n",
    "# idf(“cheaper”) = log e (3/3) + 1 => 1\n",
    "# idf(“diesel”) = log e (3/3) + 1 => 1\n",
    "# idf(“petrol”) = log e (3/3) + 1 => 1\n",
    "\n",
    "# tf idf For d1\n",
    "\n",
    "# tf-idf(“cars”) = tf(“cars”) x idf (“cars”) = 2 x 1.405465083 => 2.810930165\n",
    "# tf-idf(“cheaper”) = tf(“cheaper”) x idf (“cheaper”) = 1 x 1 => 1\n",
    "# tf-idf(“diesel”) = tf(“diesel”) x idf (“diesel”) = 1×1 => 1\n",
    "# tf-idf(“petrol”) = tf(“petrol”) x idf (“petrol”) = 1×1 => 1\n",
    "\n",
    "# normalize value d1\n",
    "\n",
    "# 2.810930165 / sqrt( 2.810930165^2 + 1^2 + 1^2 + 1^2) => 0.851354321\n",
    "# 1 / sqrt( 2.810930165^2 + 1^2 + 1^2 + 1^2) =>  0.302872811\n",
    "# 1 / sqrt( 2.810930165^2 + 1^2 + 1^2 + 1^2) => 0.302872811\n",
    "# 1 / sqrt( 2.810930165^2 + 1^2 + 1^2 + 1^2) => 0.302872811"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.transform(['cars cars car']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d1\n",
    "\n",
    "# idf(“cars”) = log e (3/2) +1 => 1.405465083\n",
    "\n",
    "# tf idf For d1\n",
    "\n",
    "# tf-idf(“cars”) = tf(“cars”) x idf (“cars”) = 2 x 1.405465083 => 2.810930165\n",
    "\n",
    "# normalize value d1\n",
    "\n",
    "# 2.810930165 / sqrt( 2.810930165^2) => 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_used = ('POLITICS', 'ENTERTAINMENT', 'WELLNESS', 'HEALTHY LIVING', 'QUEER VOICES', 'TRAVEL', 'BUSINESS', 'SPORTS', 'COMEDY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_selected = df_train[df_train['category'].isin(category_used)][['text_combine_cleaned','category']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_col = df_train_selected['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_col = {}\n",
    "\n",
    "count = 0\n",
    "for x in unique_col:\n",
    "    map_col[count] = x\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_category_id(x, unique_col):\n",
    "    count = 0\n",
    "    for col in unique_col:\n",
    "        if str(x) == str(col):\n",
    "            return count\n",
    "        count+=1 \n",
    "\n",
    "df_train_selected['category_id'] = df_train_selected['category'].apply(lambda x : to_category_id(x,unique_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, max_features=10000,\n",
    "                        stop_words='english')\n",
    "\n",
    "# We transform each complaint into a vector\n",
    "features = tfidf.fit_transform(df_train_selected['text_combine_cleaned']).toarray()\n",
    "\n",
    "labels = df_train_selected['category_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 3\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.383383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.383351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>2</td>\n",
       "      <td>0.701060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0</td>\n",
       "      <td>0.735795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2</td>\n",
       "      <td>0.733654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  fold_idx  accuracy\n",
       "0  RandomForestClassifier         0  0.383338\n",
       "1  RandomForestClassifier         1  0.383383\n",
       "2  RandomForestClassifier         2  0.383351\n",
       "3           MultinomialNB         0  0.688245\n",
       "4           MultinomialNB         1  0.712431\n",
       "5           MultinomialNB         2  0.701060\n",
       "6      LogisticRegression         0  0.735795\n",
       "7      LogisticRegression         1  0.758561\n",
       "8      LogisticRegression         2  0.733654"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combine_cleaned</th>\n",
       "      <th>category</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>of the funniest tweet about cat and dog this w...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maury will basestealing shortstop for dodger d...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>golden globe returning to nbc in january after...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>biden say u force would defend taiwan if china...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‘ beautiful and sad at the same time ’ ukraini...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                text_combine_cleaned       category  \\\n",
       "0  of the funniest tweet about cat and dog this w...         COMEDY   \n",
       "1  maury will basestealing shortstop for dodger d...         SPORTS   \n",
       "2  golden globe returning to nbc in january after...  ENTERTAINMENT   \n",
       "3  biden say u force would defend taiwan if china...       POLITICS   \n",
       "4  ‘ beautiful and sad at the same time ’ ukraini...       POLITICS   \n",
       "\n",
       "   category_id  \n",
       "0            0  \n",
       "1            1  \n",
       "2            2  \n",
       "3            3  \n",
       "4            3  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X = df_train_selected['text_combine_cleaned'] # Collection of documents\n",
    "y = df_train_selected['category'] # Target or the labels we want to predict (i.e., the 13 different complaints of products)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2), \n",
    "                        stop_words='english')\n",
    "\n",
    "fitted_vectorizer = tfidf.fit(X_train)\n",
    "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
    "\n",
    "model = LogisticRegression().fit(tfidf_vectorizer_vectors, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HEALTHY LIVING'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['eat apple loss weight']\n",
    "model.predict(fitted_vectorizer.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HEALTHY LIVING'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['apple inc loss weight']\n",
    "model.predict(fitted_vectorizer.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      BUSINESS       0.75      0.53      0.62      1229\n",
      "        COMEDY       0.70      0.39      0.50      1144\n",
      " ENTERTAINMENT       0.78      0.85      0.82      4217\n",
      "HEALTHY LIVING       0.55      0.39      0.46      1693\n",
      "      POLITICS       0.83      0.95      0.89      8790\n",
      "  QUEER VOICES       0.87      0.67      0.76      1382\n",
      "        SPORTS       0.85      0.69      0.76      1201\n",
      "        TRAVEL       0.85      0.82      0.83      1485\n",
      "      WELLNESS       0.61      0.72      0.66      2077\n",
      "\n",
      "      accuracy                           0.78     23218\n",
      "     macro avg       0.76      0.67      0.70     23218\n",
      "  weighted avg       0.78      0.78      0.77     23218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(fitted_vectorizer.transform(X_test))\n",
    "print(classification_report(y_test, y_pred, target_names=model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_df_train = []\n",
    "count = 0\n",
    "for sentence in df_train_selected['text_combine_cleaned']:\n",
    "    tagged_df_train.append(gensim.models.doc2vec.TaggedDocument(sentence.split(), [count]))\n",
    "    count+=1\n",
    "\n",
    "model_d2v = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=10, workers=cores-1)\n",
    "model_d2v.build_vocab(tagged_df_train)\n",
    "model_d2v.train(tagged_df_train, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)\n",
    "vector = model_d2v.infer_vector(df_train_selected['text_combine_cleaned'].iloc[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "res_zero = model_d2v.infer_vector(df_train_selected['text_combine_cleaned'].iloc[0].split())\n",
    "init_array = np.reshape(res_zero, (-1, 50))\n",
    "count = 0\n",
    "for word in df_train_selected['text_combine_cleaned']:\n",
    "    # print(count)\n",
    "    if count>0:\n",
    "        res_ = model_d2v.infer_vector(word.split())\n",
    "        init_array = np.vstack((init_array, res_))\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92869"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_selected['text_combine_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train_selected['category'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(init_array, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)\n",
    "labels = df_train_selected['category_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0),\n",
    "]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 3\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>fold_idx</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0</td>\n",
       "      <td>0.383338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>1</td>\n",
       "      <td>0.383383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>2</td>\n",
       "      <td>0.383351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>1</td>\n",
       "      <td>0.712431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>2</td>\n",
       "      <td>0.701060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0</td>\n",
       "      <td>0.735795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1</td>\n",
       "      <td>0.758561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>2</td>\n",
       "      <td>0.733654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  fold_idx  accuracy\n",
       "0  RandomForestClassifier         0  0.383338\n",
       "1  RandomForestClassifier         1  0.383383\n",
       "2  RandomForestClassifier         2  0.383351\n",
       "3           MultinomialNB         0  0.688245\n",
       "4           MultinomialNB         1  0.712431\n",
       "5           MultinomialNB         2  0.701060\n",
       "6      LogisticRegression         0  0.735795\n",
       "7      LogisticRegression         1  0.758561\n",
       "8      LogisticRegression         2  0.733654"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\miniconda3\\envs\\rakamin_chatbot\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "y = df_train_selected['category'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(init_array, y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state = 0)\n",
    "\n",
    "model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      BUSINESS       0.53      0.26      0.35      1229\n",
      "        COMEDY       0.37      0.08      0.13      1144\n",
      " ENTERTAINMENT       0.60      0.74      0.66      4217\n",
      "HEALTHY LIVING       0.42      0.16      0.23      1693\n",
      "      POLITICS       0.70      0.89      0.78      8790\n",
      "  QUEER VOICES       0.58      0.40      0.47      1382\n",
      "        SPORTS       0.63      0.36      0.46      1201\n",
      "        TRAVEL       0.72      0.66      0.69      1485\n",
      "      WELLNESS       0.57      0.65      0.61      2077\n",
      "\n",
      "      accuracy                           0.64     23218\n",
      "     macro avg       0.57      0.47      0.49     23218\n",
      "  weighted avg       0.61      0.64      0.61     23218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakamin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
